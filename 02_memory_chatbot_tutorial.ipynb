{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d03cc3b1",
   "metadata": {},
   "source": [
    "# 02｜为 LangGraph Chatbot 添加记忆（Checkpointing）\n",
    "\n",
    "本教程在“工具调用”基础上，使用 LangGraph 的持久化检查点（checkpointing）为聊天机器人添加多轮对话记忆。\n",
    "\n",
    "核心思路：\n",
    "- 在编译图时提供 checkpointer（例如 InMemorySaver）\n",
    "- 在调用图时提供 config = {\"configurable\": {\"thread_id\": \"...\"}}\n",
    "- 相同 thread_id 的多次调用将自动延续之前保存的 State，实现“记忆”。\n",
    "\n",
    "提示：生产环境建议替换为 SqliteSaver 或 PostgresSaver。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79046356",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd1555a2",
   "metadata": {},
   "source": [
    "## 1. 安装/升级依赖\n",
    "\n",
    "建议使用一个独立的 Python 环境（venv/conda/uv）。如需国内镜像，可自行替换 index-url。\n",
    "\n",
    "```python\n",
    "%pip install -U langgraph langsmith \"langchain[openai]\" langchain-tavily python-dotenv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d0cad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装依赖（如已安装可跳过）\n",
    "# %pip install -U langgraph langsmith \"langchain[openai]\" langchain-tavily python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac6ebb",
   "metadata": {},
   "source": [
    "## 2. 初始化提供商与聊天模型（OpenRouter 作为 OpenAI 兼容端点）\n",
    "\n",
    "本项目推荐使用 OpenRouter 以便在国内网络环境下访问多家模型：\n",
    "- 将 `OPENROUTER_API_KEY` 映射到 `OPENAI_API_KEY`\n",
    "- 设定 `base_url = \"https://openrouter.ai/api/v1\"`\n",
    "\n",
    "在 .env 中至少准备：\n",
    "- `OPENROUTER_API_KEY=sk-or-...`\n",
    "- （可选）`TAVILY_API_KEY=...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5efa22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready: openai:gpt-4o-mini via https://openrouter.ai/api/v1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载 .env 文件（仓库根目录放置 .env）\n",
    "load_dotenv()\n",
    "\n",
    "# 强制使用 OPENROUTER_API_KEY 作为 OPENAI_API_KEY（解决占位符冲突问题）\n",
    "if os.getenv(\"OPENROUTER_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.environ[\"OPENROUTER_API_KEY\"]\n",
    "\n",
    "# 选择一个模型（可替换为你在 OpenRouter 上可用的模型）\n",
    "# 例如：\"openai:gpt-4o\"、\"anthropic/claude-3.5-sonnet\" 也能通过 OpenRouter 的 openai 兼容端口使用\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "BASE_URL = os.getenv(\"OPENAI_BASE_URL\", \"https://openrouter.ai/api/v1\")\n",
    "MODEL_ID = os.getenv(\"OPENROUTER_MODEL\", \"openai:gpt-4o-mini\")\n",
    "\n",
    "llm = init_chat_model(MODEL_ID, base_url=BASE_URL)\n",
    "print(\"Model ready:\", MODEL_ID, \"via\", BASE_URL)\n",
    "\n",
    "# 简单测试连接\n",
    "response = llm.invoke(\"你好，请用中文简单介绍你自己\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a07fa4",
   "metadata": {},
   "source": [
    "## 3. 定义 State 并构建支持工具的图\n",
    "\n",
    "我们创建一个包含 `messages` 的 State（使用 `add_messages` reducer），并添加 TavilySearch 工具。\n",
    "- `chatbot` 节点：调用带工具的 llm 生成回复\n",
    "- `tools` 节点：执行工具调用（如搜索）\n",
    "- 条件边：根据模型输出是否包含 tool call 来路由到 `tools` 或结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4aabf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph nodes: <langgraph.graph.state.StateGraph object at 0x11586f7f0>\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_tavily import TavilySearch\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# 定义 State\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# 初始化图构建器\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# 准备 Tavily 搜索工具（需要 TAVILY_API_KEY）\n",
    "# 可在 .env 中设置 TAVILY_API_KEY，或直接在此处设置 os.environ[\"TAVILY_API_KEY\"]\n",
    "search_tool = TavilySearch(max_results=2)\n",
    "tools = [search_tool]\n",
    "\n",
    "# 将工具绑定到 LLM（使其具备 tool calling 能力）\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# 定义聊天节点\n",
    "\n",
    "def chatbot(state: State):\n",
    "    # 模型根据对话状态生成回复（如需要会提出 tool call）\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "# 注册节点\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# 工具节点（预构建 ToolNode 可直接执行工具）\n",
    "tool_node = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "# 条件边：根据模型输出决定是否调用工具\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "\n",
    "# 工具执行完毕后回到聊天节点继续处理\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "# 入口节点\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "print(\"Graph nodes:\", graph_builder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a55cea",
   "metadata": {},
   "source": [
    "## 4. 添加 MemorySaver 并带检查点编译\n",
    "\n",
    "使用内存型检查点 InMemorySaver()，并在编译时传入 `checkpointer=memory`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f935a02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph compiled with InMemorySaver checkpointer.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "memory = InMemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "print(\"Graph compiled with InMemorySaver checkpointer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f51addd",
   "metadata": {},
   "source": [
    "## 5. 以 thread_id=1 开始一次对话（stream values）\n",
    "\n",
    "注意：config 作为 `graph.stream` 或 `graph.invoke` 的第二个位置参数传入，不嵌在 inputs 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f31838ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi there! My name is Will.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Will! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "user_input = \"Hi there! My name is Will.\"\n",
    "\n",
    "# config 是第二个位置参数，传给 stream() 或 invoke()！\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b2cc37",
   "metadata": {},
   "source": [
    "### 📖 深入理解 `graph.stream` 和 `event`\n",
    "\n",
    "#### 1. `graph.stream()` 方法\n",
    "```python\n",
    "graph.stream(inputs, config, stream_mode=\"values\")\n",
    "```\n",
    "\n",
    "- **返回值**: 一个 Python **生成器 (generator)**，可以逐步迭代图的执行过程\n",
    "- **参数说明**:\n",
    "  - `inputs`: 输入数据，格式为 `{\"messages\": [...]}`\n",
    "  - `config`: 配置信息，包含 `thread_id` 等\n",
    "  - `stream_mode=\"values\"`: 流模式，返回每个节点执行后的完整 State\n",
    "\n",
    "#### 2. `event` 对象\n",
    "每次迭代 `graph.stream()` 时得到的对象，包含：\n",
    "\n",
    "- **类型**: `dict` 字典\n",
    "- **内容**: 当前 State 的快照，键通常是 `\"messages\"`\n",
    "- **含义**: 图中某个节点执行完毕后的状态\n",
    "\n",
    "#### 3. 执行流程解析\n",
    "\n",
    "1. **Event 1**: 用户消息 + AI 回复（chatbot 节点执行完毕）\n",
    "2. **Event 2**: 如果需要工具调用，会有更多 events\n",
    "3. **最终**: 每个 event 都包含完整的对话历史\n",
    "\n",
    "#### 4. 为什么使用 `stream`？\n",
    "\n",
    "- **实时反馈**: 可以看到图的逐步执行过程\n",
    "- **调试友好**: 了解每个节点的输出\n",
    "- **用户体验**: 可以实现类似 ChatGPT 的逐字显示效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "191d8280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 对比不同的 stream_mode\n",
    "# print(\"=== 对比不同 stream_mode ===\")\n",
    "\n",
    "# config = {\"configurable\": {\"thread_id\": \"demo_modes\"}}\n",
    "# user_msg = {\"messages\": [{\"role\": \"user\", \"content\": \"Hello! What's 2+2?\"}]}\n",
    "\n",
    "# print(\"\\n1. stream_mode='values' (默认)\")\n",
    "# print(\"返回完整的 State 值:\")\n",
    "# for i, event in enumerate(graph.stream(user_msg, config, stream_mode=\"values\"), 1):\n",
    "#     print(f\"Event {i}: {list(event.keys())} - {len(event['messages'])} messages\")\n",
    "#     print(f\"最新消息: {event['messages'][-1].content[:30]}...\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# # 重置会话\n",
    "# config2 = {\"configurable\": {\"thread_id\": \"demo_modes2\"}} \n",
    "\n",
    "# print(\"\\n2. stream_mode='updates'\")\n",
    "# print(\"返回每个节点的增量更新:\")\n",
    "# for i, event in enumerate(graph.stream(user_msg, config2, stream_mode=\"updates\"), 1):\n",
    "#     print(f\"Event {i}: {event}\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02feb9c9",
   "metadata": {},
   "source": [
    "### 🎯 总结：为什么要用 `event[\"messages\"][-1].pretty_print()`？\n",
    "\n",
    "在循环中：\n",
    "```python\n",
    "for event in graph.stream(...):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "```\n",
    "\n",
    "**分解理解：**\n",
    "\n",
    "1. **`graph.stream(...)`**: 返回生成器，每次 yield 一个 event（图执行的中间状态）\n",
    "\n",
    "2. **`event`**: 字典类型，包含当前的 State，键为 `\"messages\"`\n",
    "\n",
    "3. **`event[\"messages\"]`**: 获取消息列表，包含完整对话历史\n",
    "\n",
    "4. **`event[\"messages\"][-1]`**: 获取最新的一条消息（用户输入或AI回复）\n",
    "\n",
    "5. **`.pretty_print()`**: LangChain 消息对象的方法，美化打印消息内容\n",
    "\n",
    "**为什么这样设计？**\n",
    "- 可以实时看到图的执行进度\n",
    "- 避免等待整个对话完成才看到结果  \n",
    "- 支持长时间运行的复杂工作流\n",
    "- 便于调试和监控图的执行状态"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cee403d",
   "metadata": {},
   "source": [
    "## 6. 同一会话继续追问（记忆验证）\n",
    "\n",
    "继续使用 `thread_id=1`，机器人应能记住你的姓名等上下文信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24d22daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Remember my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, I remember your name, Will! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Remember my name?\"\n",
    "\n",
    "# 使用相同的 config（thread_id=1），应该能记住之前的对话\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162f9296",
   "metadata": {},
   "source": [
    "## 7. 切换为 thread_id=2（无先前上下文）\n",
    "\n",
    "仅改变 `thread_id` 即可观察到没有先前的记忆上下文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ed850cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Remember my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't have the ability to recall personal information or previous interactions. Could you please remind me of your name?\n"
     ]
    }
   ],
   "source": [
    "config_isolated = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# 使用不同的 thread_id，应该没有之前的对话记忆\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Remember my name?\"}]},\n",
    "    config_isolated,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452bd2e7",
   "metadata": {},
   "source": [
    "## 8. 检查保存的 State（get_state）\n",
    "\n",
    "可以随时用 `graph.get_state(config)` 查看某个会话（thread）的最新快照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e814414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(StateSnapshot(values={'messages': [HumanMessage(content='Introduce yourself briefly.', additional_kwargs={}, response_metadata={}, id='af1ae085-a23f-4cc3-9ef6-1bf2edd82bc8'), AIMessage(content='I’m an AI language model created to assist with a wide range of inquiries, providing information, answering questions, and engaging in conversations across various topics. My goal is to offer accurate and helpful responses to meet your needs!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 1272, 'total_tokens': 1317, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 1152}}, 'model_name': 'openai/gpt-4o-mini', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'gen-1755012620-14a1ihLsPhkHZHM4Zt8l', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--811787a1-f847-4515-854b-2ced692faf4c-0', usage_metadata={'input_tokens': 1272, 'output_tokens': 45, 'total_tokens': 1317, 'input_token_details': {'cache_read': 1152}, 'output_token_details': {'reasoning': 0}})]}, next=(), config={'configurable': {'thread_id': 'demo', 'checkpoint_ns': '', 'checkpoint_id': '1f077914-25e5-62ee-8001-fab275a51140'}}, metadata={'source': 'loop', 'step': 1, 'parents': {}}, created_at='2025-08-12T15:30:21.592642+00:00', parent_config={'configurable': {'thread_id': 'demo', 'checkpoint_ns': '', 'checkpoint_id': '1f077914-1857-65d2-8000-2179b0a6876f'}}, tasks=(), interrupts=()),\n",
       " 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "snapshot, len(snapshot.values.get(\"messages\", []))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d224e8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 查看下一步节点（如果当前已结束，next 为空）\n",
    "snapshot.next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbadea8",
   "metadata": {},
   "source": [
    "snapshot.next  # (since the graph ended this turn, `next` is empty. If you fetch a state from within a graph invocation, next tells which node will execute next)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a1e754",
   "metadata": {},
   "source": [
    "## 9. 可选：使用 SQLite/Postgres 作为持久化检查点\n",
    "\n",
    "生产环境建议使用持久化存储：\n",
    "\n",
    "- SqliteSaver（本地/轻量）\n",
    "- PostgresSaver（生产/云端）\n",
    "\n",
    "示例（仅演示，不会直接执行）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "199a222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例：SqliteSaver\n",
    "# from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "# sqlite_memory = SqliteSaver.from_conn_string(\"chat_memory.db\")\n",
    "# graph = graph_builder.compile(checkpointer=sqlite_memory)\n",
    "\n",
    "# 示例：PostgresSaver\n",
    "# from langgraph.checkpoint.postgres import PostgresSaver\n",
    "# PG_DSN = os.getenv(\"POSTGRES_DSN\", \"postgresql+psycopg://user:pass@host:5432/dbname\")\n",
    "# pg_memory = PostgresSaver.from_conn_string(PG_DSN)\n",
    "# graph = graph_builder.compile(checkpointer=pg_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91243f96",
   "metadata": {},
   "source": [
    "### Bonus：整合代码（与教程片段对应）\n",
    "\n",
    "以下单元格汇总核心代码，便于快速复制使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07f62faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Introduce yourself briefly.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I’m an AI language model created to assist with a wide range of inquiries, providing information, answering questions, and engaging in conversations across various topics. My goal is to offer accurate and helpful responses to meet your needs!\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_tavily import TavilySearch\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# 环境准备\n",
    "load_dotenv()\n",
    "if os.getenv(\"OPENROUTER_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.environ[\"OPENROUTER_API_KEY\"]\n",
    "\n",
    "BASE_URL = os.getenv(\"OPENAI_BASE_URL\", \"https://openrouter.ai/api/v1\")\n",
    "MODEL_ID = os.getenv(\"OPENROUTER_MODEL\", \"openai:gpt-4o-mini\")\n",
    "\n",
    "llm = init_chat_model(MODEL_ID, base_url=BASE_URL)\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "tool = TavilySearch(max_results=2)\n",
    "tools = [tool]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "memory = InMemorySaver()\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "# 简单测试\n",
    "config = {\"configurable\": {\"thread_id\": \"demo\"}}\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Introduce yourself briefly.\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
