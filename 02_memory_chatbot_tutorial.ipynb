{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d03cc3b1",
   "metadata": {},
   "source": [
    "# 02ï½œä¸º LangGraph Chatbot æ·»åŠ è®°å¿†ï¼ˆCheckpointingï¼‰\n",
    "\n",
    "æœ¬æ•™ç¨‹åœ¨â€œå·¥å…·è°ƒç”¨â€åŸºç¡€ä¸Šï¼Œä½¿ç”¨ LangGraph çš„æŒä¹…åŒ–æ£€æŸ¥ç‚¹ï¼ˆcheckpointingï¼‰ä¸ºèŠå¤©æœºå™¨äººæ·»åŠ å¤šè½®å¯¹è¯è®°å¿†ã€‚\n",
    "\n",
    "æ ¸å¿ƒæ€è·¯ï¼š\n",
    "- åœ¨ç¼–è¯‘å›¾æ—¶æä¾› checkpointerï¼ˆä¾‹å¦‚ InMemorySaverï¼‰\n",
    "- åœ¨è°ƒç”¨å›¾æ—¶æä¾› config = {\"configurable\": {\"thread_id\": \"...\"}}\n",
    "- ç›¸åŒ thread_id çš„å¤šæ¬¡è°ƒç”¨å°†è‡ªåŠ¨å»¶ç»­ä¹‹å‰ä¿å­˜çš„ Stateï¼Œå®ç°â€œè®°å¿†â€ã€‚\n",
    "\n",
    "æç¤ºï¼šç”Ÿäº§ç¯å¢ƒå»ºè®®æ›¿æ¢ä¸º SqliteSaver æˆ– PostgresSaverã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79046356",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd1555a2",
   "metadata": {},
   "source": [
    "## 1. å®‰è£…/å‡çº§ä¾èµ–\n",
    "\n",
    "å»ºè®®ä½¿ç”¨ä¸€ä¸ªç‹¬ç«‹çš„ Python ç¯å¢ƒï¼ˆvenv/conda/uvï¼‰ã€‚å¦‚éœ€å›½å†…é•œåƒï¼Œå¯è‡ªè¡Œæ›¿æ¢ index-urlã€‚\n",
    "\n",
    "```python\n",
    "%pip install -U langgraph langsmith \"langchain[openai]\" langchain-tavily python-dotenv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d0cad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…ä¾èµ–ï¼ˆå¦‚å·²å®‰è£…å¯è·³è¿‡ï¼‰\n",
    "# %pip install -U langgraph langsmith \"langchain[openai]\" langchain-tavily python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac6ebb",
   "metadata": {},
   "source": [
    "## 2. åˆå§‹åŒ–æä¾›å•†ä¸èŠå¤©æ¨¡å‹ï¼ˆOpenRouter ä½œä¸º OpenAI å…¼å®¹ç«¯ç‚¹ï¼‰\n",
    "\n",
    "æœ¬é¡¹ç›®æ¨èä½¿ç”¨ OpenRouter ä»¥ä¾¿åœ¨å›½å†…ç½‘ç»œç¯å¢ƒä¸‹è®¿é—®å¤šå®¶æ¨¡å‹ï¼š\n",
    "- å°† `OPENROUTER_API_KEY` æ˜ å°„åˆ° `OPENAI_API_KEY`\n",
    "- è®¾å®š `base_url = \"https://openrouter.ai/api/v1\"`\n",
    "\n",
    "åœ¨ .env ä¸­è‡³å°‘å‡†å¤‡ï¼š\n",
    "- `OPENROUTER_API_KEY=sk-or-...`\n",
    "- ï¼ˆå¯é€‰ï¼‰`TAVILY_API_KEY=...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5efa22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready: openai:gpt-4o-mini via https://openrouter.ai/api/v1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# åŠ è½½ .env æ–‡ä»¶ï¼ˆä»“åº“æ ¹ç›®å½•æ”¾ç½® .envï¼‰\n",
    "load_dotenv()\n",
    "\n",
    "# å¼ºåˆ¶ä½¿ç”¨ OPENROUTER_API_KEY ä½œä¸º OPENAI_API_KEYï¼ˆè§£å†³å ä½ç¬¦å†²çªé—®é¢˜ï¼‰\n",
    "if os.getenv(\"OPENROUTER_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.environ[\"OPENROUTER_API_KEY\"]\n",
    "\n",
    "# é€‰æ‹©ä¸€ä¸ªæ¨¡å‹ï¼ˆå¯æ›¿æ¢ä¸ºä½ åœ¨ OpenRouter ä¸Šå¯ç”¨çš„æ¨¡å‹ï¼‰\n",
    "# ä¾‹å¦‚ï¼š\"openai:gpt-4o\"ã€\"anthropic/claude-3.5-sonnet\" ä¹Ÿèƒ½é€šè¿‡ OpenRouter çš„ openai å…¼å®¹ç«¯å£ä½¿ç”¨\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "BASE_URL = os.getenv(\"OPENAI_BASE_URL\", \"https://openrouter.ai/api/v1\")\n",
    "MODEL_ID = os.getenv(\"OPENROUTER_MODEL\", \"openai:gpt-4o-mini\")\n",
    "\n",
    "llm = init_chat_model(MODEL_ID, base_url=BASE_URL)\n",
    "print(\"Model ready:\", MODEL_ID, \"via\", BASE_URL)\n",
    "\n",
    "# ç®€å•æµ‹è¯•è¿æ¥\n",
    "response = llm.invoke(\"ä½ å¥½ï¼Œè¯·ç”¨ä¸­æ–‡ç®€å•ä»‹ç»ä½ è‡ªå·±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a07fa4",
   "metadata": {},
   "source": [
    "## 3. å®šä¹‰ State å¹¶æ„å»ºæ”¯æŒå·¥å…·çš„å›¾\n",
    "\n",
    "æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŒ…å« `messages` çš„ Stateï¼ˆä½¿ç”¨ `add_messages` reducerï¼‰ï¼Œå¹¶æ·»åŠ  TavilySearch å·¥å…·ã€‚\n",
    "- `chatbot` èŠ‚ç‚¹ï¼šè°ƒç”¨å¸¦å·¥å…·çš„ llm ç”Ÿæˆå›å¤\n",
    "- `tools` èŠ‚ç‚¹ï¼šæ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆå¦‚æœç´¢ï¼‰\n",
    "- æ¡ä»¶è¾¹ï¼šæ ¹æ®æ¨¡å‹è¾“å‡ºæ˜¯å¦åŒ…å« tool call æ¥è·¯ç”±åˆ° `tools` æˆ–ç»“æŸ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4aabf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph nodes: <langgraph.graph.state.StateGraph object at 0x11586f7f0>\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_tavily import TavilySearch\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# å®šä¹‰ State\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# åˆå§‹åŒ–å›¾æ„å»ºå™¨\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# å‡†å¤‡ Tavily æœç´¢å·¥å…·ï¼ˆéœ€è¦ TAVILY_API_KEYï¼‰\n",
    "# å¯åœ¨ .env ä¸­è®¾ç½® TAVILY_API_KEYï¼Œæˆ–ç›´æ¥åœ¨æ­¤å¤„è®¾ç½® os.environ[\"TAVILY_API_KEY\"]\n",
    "search_tool = TavilySearch(max_results=2)\n",
    "tools = [search_tool]\n",
    "\n",
    "# å°†å·¥å…·ç»‘å®šåˆ° LLMï¼ˆä½¿å…¶å…·å¤‡ tool calling èƒ½åŠ›ï¼‰\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# å®šä¹‰èŠå¤©èŠ‚ç‚¹\n",
    "\n",
    "def chatbot(state: State):\n",
    "    # æ¨¡å‹æ ¹æ®å¯¹è¯çŠ¶æ€ç”Ÿæˆå›å¤ï¼ˆå¦‚éœ€è¦ä¼šæå‡º tool callï¼‰\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "# æ³¨å†ŒèŠ‚ç‚¹\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# å·¥å…·èŠ‚ç‚¹ï¼ˆé¢„æ„å»º ToolNode å¯ç›´æ¥æ‰§è¡Œå·¥å…·ï¼‰\n",
    "tool_node = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "# æ¡ä»¶è¾¹ï¼šæ ¹æ®æ¨¡å‹è¾“å‡ºå†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "\n",
    "# å·¥å…·æ‰§è¡Œå®Œæ¯•åå›åˆ°èŠå¤©èŠ‚ç‚¹ç»§ç»­å¤„ç†\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "# å…¥å£èŠ‚ç‚¹\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "print(\"Graph nodes:\", graph_builder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a55cea",
   "metadata": {},
   "source": [
    "## 4. æ·»åŠ  MemorySaver å¹¶å¸¦æ£€æŸ¥ç‚¹ç¼–è¯‘\n",
    "\n",
    "ä½¿ç”¨å†…å­˜å‹æ£€æŸ¥ç‚¹ InMemorySaver()ï¼Œå¹¶åœ¨ç¼–è¯‘æ—¶ä¼ å…¥ `checkpointer=memory`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f935a02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph compiled with InMemorySaver checkpointer.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "memory = InMemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "print(\"Graph compiled with InMemorySaver checkpointer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f51addd",
   "metadata": {},
   "source": [
    "## 5. ä»¥ thread_id=1 å¼€å§‹ä¸€æ¬¡å¯¹è¯ï¼ˆstream valuesï¼‰\n",
    "\n",
    "æ³¨æ„ï¼šconfig ä½œä¸º `graph.stream` æˆ– `graph.invoke` çš„ç¬¬äºŒä¸ªä½ç½®å‚æ•°ä¼ å…¥ï¼Œä¸åµŒåœ¨ inputs ä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f31838ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi there! My name is Will.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Will! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "user_input = \"Hi there! My name is Will.\"\n",
    "\n",
    "# config æ˜¯ç¬¬äºŒä¸ªä½ç½®å‚æ•°ï¼Œä¼ ç»™ stream() æˆ– invoke()ï¼\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b2cc37",
   "metadata": {},
   "source": [
    "### ğŸ“– æ·±å…¥ç†è§£ `graph.stream` å’Œ `event`\n",
    "\n",
    "#### 1. `graph.stream()` æ–¹æ³•\n",
    "```python\n",
    "graph.stream(inputs, config, stream_mode=\"values\")\n",
    "```\n",
    "\n",
    "- **è¿”å›å€¼**: ä¸€ä¸ª Python **ç”Ÿæˆå™¨ (generator)**ï¼Œå¯ä»¥é€æ­¥è¿­ä»£å›¾çš„æ‰§è¡Œè¿‡ç¨‹\n",
    "- **å‚æ•°è¯´æ˜**:\n",
    "  - `inputs`: è¾“å…¥æ•°æ®ï¼Œæ ¼å¼ä¸º `{\"messages\": [...]}`\n",
    "  - `config`: é…ç½®ä¿¡æ¯ï¼ŒåŒ…å« `thread_id` ç­‰\n",
    "  - `stream_mode=\"values\"`: æµæ¨¡å¼ï¼Œè¿”å›æ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œåçš„å®Œæ•´ State\n",
    "\n",
    "#### 2. `event` å¯¹è±¡\n",
    "æ¯æ¬¡è¿­ä»£ `graph.stream()` æ—¶å¾—åˆ°çš„å¯¹è±¡ï¼ŒåŒ…å«ï¼š\n",
    "\n",
    "- **ç±»å‹**: `dict` å­—å…¸\n",
    "- **å†…å®¹**: å½“å‰ State çš„å¿«ç…§ï¼Œé”®é€šå¸¸æ˜¯ `\"messages\"`\n",
    "- **å«ä¹‰**: å›¾ä¸­æŸä¸ªèŠ‚ç‚¹æ‰§è¡Œå®Œæ¯•åçš„çŠ¶æ€\n",
    "\n",
    "#### 3. æ‰§è¡Œæµç¨‹è§£æ\n",
    "\n",
    "1. **Event 1**: ç”¨æˆ·æ¶ˆæ¯ + AI å›å¤ï¼ˆchatbot èŠ‚ç‚¹æ‰§è¡Œå®Œæ¯•ï¼‰\n",
    "2. **Event 2**: å¦‚æœéœ€è¦å·¥å…·è°ƒç”¨ï¼Œä¼šæœ‰æ›´å¤š events\n",
    "3. **æœ€ç»ˆ**: æ¯ä¸ª event éƒ½åŒ…å«å®Œæ•´çš„å¯¹è¯å†å²\n",
    "\n",
    "#### 4. ä¸ºä»€ä¹ˆä½¿ç”¨ `stream`ï¼Ÿ\n",
    "\n",
    "- **å®æ—¶åé¦ˆ**: å¯ä»¥çœ‹åˆ°å›¾çš„é€æ­¥æ‰§è¡Œè¿‡ç¨‹\n",
    "- **è°ƒè¯•å‹å¥½**: äº†è§£æ¯ä¸ªèŠ‚ç‚¹çš„è¾“å‡º\n",
    "- **ç”¨æˆ·ä½“éªŒ**: å¯ä»¥å®ç°ç±»ä¼¼ ChatGPT çš„é€å­—æ˜¾ç¤ºæ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "191d8280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # å¯¹æ¯”ä¸åŒçš„ stream_mode\n",
    "# print(\"=== å¯¹æ¯”ä¸åŒ stream_mode ===\")\n",
    "\n",
    "# config = {\"configurable\": {\"thread_id\": \"demo_modes\"}}\n",
    "# user_msg = {\"messages\": [{\"role\": \"user\", \"content\": \"Hello! What's 2+2?\"}]}\n",
    "\n",
    "# print(\"\\n1. stream_mode='values' (é»˜è®¤)\")\n",
    "# print(\"è¿”å›å®Œæ•´çš„ State å€¼:\")\n",
    "# for i, event in enumerate(graph.stream(user_msg, config, stream_mode=\"values\"), 1):\n",
    "#     print(f\"Event {i}: {list(event.keys())} - {len(event['messages'])} messages\")\n",
    "#     print(f\"æœ€æ–°æ¶ˆæ¯: {event['messages'][-1].content[:30]}...\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# # é‡ç½®ä¼šè¯\n",
    "# config2 = {\"configurable\": {\"thread_id\": \"demo_modes2\"}} \n",
    "\n",
    "# print(\"\\n2. stream_mode='updates'\")\n",
    "# print(\"è¿”å›æ¯ä¸ªèŠ‚ç‚¹çš„å¢é‡æ›´æ–°:\")\n",
    "# for i, event in enumerate(graph.stream(user_msg, config2, stream_mode=\"updates\"), 1):\n",
    "#     print(f\"Event {i}: {event}\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02feb9c9",
   "metadata": {},
   "source": [
    "### ğŸ¯ æ€»ç»“ï¼šä¸ºä»€ä¹ˆè¦ç”¨ `event[\"messages\"][-1].pretty_print()`ï¼Ÿ\n",
    "\n",
    "åœ¨å¾ªç¯ä¸­ï¼š\n",
    "```python\n",
    "for event in graph.stream(...):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "```\n",
    "\n",
    "**åˆ†è§£ç†è§£ï¼š**\n",
    "\n",
    "1. **`graph.stream(...)`**: è¿”å›ç”Ÿæˆå™¨ï¼Œæ¯æ¬¡ yield ä¸€ä¸ª eventï¼ˆå›¾æ‰§è¡Œçš„ä¸­é—´çŠ¶æ€ï¼‰\n",
    "\n",
    "2. **`event`**: å­—å…¸ç±»å‹ï¼ŒåŒ…å«å½“å‰çš„ Stateï¼Œé”®ä¸º `\"messages\"`\n",
    "\n",
    "3. **`event[\"messages\"]`**: è·å–æ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«å®Œæ•´å¯¹è¯å†å²\n",
    "\n",
    "4. **`event[\"messages\"][-1]`**: è·å–æœ€æ–°çš„ä¸€æ¡æ¶ˆæ¯ï¼ˆç”¨æˆ·è¾“å…¥æˆ–AIå›å¤ï¼‰\n",
    "\n",
    "5. **`.pretty_print()`**: LangChain æ¶ˆæ¯å¯¹è±¡çš„æ–¹æ³•ï¼Œç¾åŒ–æ‰“å°æ¶ˆæ¯å†…å®¹\n",
    "\n",
    "**ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ**\n",
    "- å¯ä»¥å®æ—¶çœ‹åˆ°å›¾çš„æ‰§è¡Œè¿›åº¦\n",
    "- é¿å…ç­‰å¾…æ•´ä¸ªå¯¹è¯å®Œæˆæ‰çœ‹åˆ°ç»“æœ  \n",
    "- æ”¯æŒé•¿æ—¶é—´è¿è¡Œçš„å¤æ‚å·¥ä½œæµ\n",
    "- ä¾¿äºè°ƒè¯•å’Œç›‘æ§å›¾çš„æ‰§è¡ŒçŠ¶æ€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cee403d",
   "metadata": {},
   "source": [
    "## 6. åŒä¸€ä¼šè¯ç»§ç»­è¿½é—®ï¼ˆè®°å¿†éªŒè¯ï¼‰\n",
    "\n",
    "ç»§ç»­ä½¿ç”¨ `thread_id=1`ï¼Œæœºå™¨äººåº”èƒ½è®°ä½ä½ çš„å§“åç­‰ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24d22daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Remember my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, I remember your name, Will! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Remember my name?\"\n",
    "\n",
    "# ä½¿ç”¨ç›¸åŒçš„ configï¼ˆthread_id=1ï¼‰ï¼Œåº”è¯¥èƒ½è®°ä½ä¹‹å‰çš„å¯¹è¯\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162f9296",
   "metadata": {},
   "source": [
    "## 7. åˆ‡æ¢ä¸º thread_id=2ï¼ˆæ— å…ˆå‰ä¸Šä¸‹æ–‡ï¼‰\n",
    "\n",
    "ä»…æ”¹å˜ `thread_id` å³å¯è§‚å¯Ÿåˆ°æ²¡æœ‰å…ˆå‰çš„è®°å¿†ä¸Šä¸‹æ–‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ed850cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Remember my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't have the ability to recall personal information or previous interactions. Could you please remind me of your name?\n"
     ]
    }
   ],
   "source": [
    "config_isolated = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# ä½¿ç”¨ä¸åŒçš„ thread_idï¼Œåº”è¯¥æ²¡æœ‰ä¹‹å‰çš„å¯¹è¯è®°å¿†\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Remember my name?\"}]},\n",
    "    config_isolated,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452bd2e7",
   "metadata": {},
   "source": [
    "## 8. æ£€æŸ¥ä¿å­˜çš„ Stateï¼ˆget_stateï¼‰\n",
    "\n",
    "å¯ä»¥éšæ—¶ç”¨ `graph.get_state(config)` æŸ¥çœ‹æŸä¸ªä¼šè¯ï¼ˆthreadï¼‰çš„æœ€æ–°å¿«ç…§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e814414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(StateSnapshot(values={'messages': [HumanMessage(content='Introduce yourself briefly.', additional_kwargs={}, response_metadata={}, id='af1ae085-a23f-4cc3-9ef6-1bf2edd82bc8'), AIMessage(content='Iâ€™m an AI language model created to assist with a wide range of inquiries, providing information, answering questions, and engaging in conversations across various topics. My goal is to offer accurate and helpful responses to meet your needs!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 1272, 'total_tokens': 1317, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 1152}}, 'model_name': 'openai/gpt-4o-mini', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'gen-1755012620-14a1ihLsPhkHZHM4Zt8l', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--811787a1-f847-4515-854b-2ced692faf4c-0', usage_metadata={'input_tokens': 1272, 'output_tokens': 45, 'total_tokens': 1317, 'input_token_details': {'cache_read': 1152}, 'output_token_details': {'reasoning': 0}})]}, next=(), config={'configurable': {'thread_id': 'demo', 'checkpoint_ns': '', 'checkpoint_id': '1f077914-25e5-62ee-8001-fab275a51140'}}, metadata={'source': 'loop', 'step': 1, 'parents': {}}, created_at='2025-08-12T15:30:21.592642+00:00', parent_config={'configurable': {'thread_id': 'demo', 'checkpoint_ns': '', 'checkpoint_id': '1f077914-1857-65d2-8000-2179b0a6876f'}}, tasks=(), interrupts=()),\n",
       " 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "snapshot, len(snapshot.values.get(\"messages\", []))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d224e8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# æŸ¥çœ‹ä¸‹ä¸€æ­¥èŠ‚ç‚¹ï¼ˆå¦‚æœå½“å‰å·²ç»“æŸï¼Œnext ä¸ºç©ºï¼‰\n",
    "snapshot.next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbadea8",
   "metadata": {},
   "source": [
    "snapshot.next  # (since the graph ended this turn, `next` is empty. If you fetch a state from within a graph invocation, next tells which node will execute next)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a1e754",
   "metadata": {},
   "source": [
    "## 9. å¯é€‰ï¼šä½¿ç”¨ SQLite/Postgres ä½œä¸ºæŒä¹…åŒ–æ£€æŸ¥ç‚¹\n",
    "\n",
    "ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨æŒä¹…åŒ–å­˜å‚¨ï¼š\n",
    "\n",
    "- SqliteSaverï¼ˆæœ¬åœ°/è½»é‡ï¼‰\n",
    "- PostgresSaverï¼ˆç”Ÿäº§/äº‘ç«¯ï¼‰\n",
    "\n",
    "ç¤ºä¾‹ï¼ˆä»…æ¼”ç¤ºï¼Œä¸ä¼šç›´æ¥æ‰§è¡Œï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "199a222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¤ºä¾‹ï¼šSqliteSaver\n",
    "# from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "# sqlite_memory = SqliteSaver.from_conn_string(\"chat_memory.db\")\n",
    "# graph = graph_builder.compile(checkpointer=sqlite_memory)\n",
    "\n",
    "# ç¤ºä¾‹ï¼šPostgresSaver\n",
    "# from langgraph.checkpoint.postgres import PostgresSaver\n",
    "# PG_DSN = os.getenv(\"POSTGRES_DSN\", \"postgresql+psycopg://user:pass@host:5432/dbname\")\n",
    "# pg_memory = PostgresSaver.from_conn_string(PG_DSN)\n",
    "# graph = graph_builder.compile(checkpointer=pg_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91243f96",
   "metadata": {},
   "source": [
    "### Bonusï¼šæ•´åˆä»£ç ï¼ˆä¸æ•™ç¨‹ç‰‡æ®µå¯¹åº”ï¼‰\n",
    "\n",
    "ä»¥ä¸‹å•å…ƒæ ¼æ±‡æ€»æ ¸å¿ƒä»£ç ï¼Œä¾¿äºå¿«é€Ÿå¤åˆ¶ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07f62faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Introduce yourself briefly.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Iâ€™m an AI language model created to assist with a wide range of inquiries, providing information, answering questions, and engaging in conversations across various topics. My goal is to offer accurate and helpful responses to meet your needs!\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_tavily import TavilySearch\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# ç¯å¢ƒå‡†å¤‡\n",
    "load_dotenv()\n",
    "if os.getenv(\"OPENROUTER_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.environ[\"OPENROUTER_API_KEY\"]\n",
    "\n",
    "BASE_URL = os.getenv(\"OPENAI_BASE_URL\", \"https://openrouter.ai/api/v1\")\n",
    "MODEL_ID = os.getenv(\"OPENROUTER_MODEL\", \"openai:gpt-4o-mini\")\n",
    "\n",
    "llm = init_chat_model(MODEL_ID, base_url=BASE_URL)\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "tool = TavilySearch(max_results=2)\n",
    "tools = [tool]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "memory = InMemorySaver()\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "# ç®€å•æµ‹è¯•\n",
    "config = {\"configurable\": {\"thread_id\": \"demo\"}}\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Introduce yourself briefly.\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
